{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "!pip install torchsummary\n",
    "\n",
    "!pip install Pillow==4.0.0\n",
    "!pip install PIL\n",
    "!pip install image\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math;\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/code for convolocean\")\n",
    "os.getcwd()\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images = pickle.load(open(os.getcwd()+\"/pr_train_images.pkl\",\"rb\"))\n",
    "train_labels = pickle.load(open(os.getcwd()+\"/pr_labels.pkl\",\"rb\"))\n",
    "\n",
    "test_images  = pickle.load(open(os.getcwd()+\"/pr_test_img.pkl\",\"rb\"))\n",
    "test_labels  = pickle.load(open(os.getcwd()+\"/pr_test_labels.pkl\",\"rb\"))\n",
    "\n",
    "val_images   = pickle.load(open(os.getcwd()+\"/pr_val_img.pkl\",\"rb\"))\n",
    "val_fname    = pickle.load(open(os.getcwd()+\"/val_nam.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_images=[]\n",
    "n_test_labels=[]\n",
    "i=0\n",
    "for image in test_images:\n",
    "    n_test_images.append(image)\n",
    "    n_test_images.append(transforms.functional.rotate(image, 90))\n",
    "    n_test_images.append(transforms.functional.rotate(image, 180))\n",
    "    n_test_images.append(transforms.functional.rotate(image, 270))\n",
    "    n_test_labels.extend(4*[test_labels[i]])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val_images=[]\n",
    "n_val_fname=[]\n",
    "i=0\n",
    "for image in val_images:\n",
    "    n_val_images.append(image)\n",
    "    n_val_images.append(transforms.functional.rotate(image, 90))\n",
    "    n_val_images.append(transforms.functional.rotate(image, 180))\n",
    "    n_val_images.append(transforms.functional.rotate(image, 270))\n",
    "    #n_val_fname.extend(4*[val_fname[i]])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingImagesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.images[index]\n",
    "        single_label = self.labels[index]\n",
    "        \n",
    "        if self.transform != None:\n",
    "            #image to tensor\n",
    "            tensor = self.transform(single_image)\n",
    "            return (tensor, single_label)\n",
    "        #return image without turning it to tensor\n",
    "        #probably wrong\n",
    "        return (single_image, single_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ValidationImagesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.images[index]\n",
    "        if self.transform:\n",
    "            #image to tensor\n",
    "            tensor = self.transform(single_image)\n",
    "            return tensor\n",
    "        return single_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_images, train_lbs, test_images=None, test_lbs = None, batch_size = 32):\n",
    "    \n",
    "    train_dataset = TrainingImagesDataset(train_images, train_lbs, transform=transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(p=0.5),transforms.RandomRotation(degrees=360), transforms.ToTensor()]))\n",
    "    \n",
    "    if test_images and test_lbs:\n",
    "        print(\"here\")\n",
    "        test_dataset = TrainingImagesDataset(test_images, test_lbs, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "    elif test_images:\n",
    "        test_dataset = ValidationImagesDataset(test_images, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "    else:\n",
    "        test_dataset = []\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 0)    \n",
    "    \n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lass CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, 5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 48, 3, padding=1)\n",
    "        #self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(48,32,3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 121)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "ocean = CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(ocean.parameters(), lr = 0.001, momentum = 0.9, weight_decay=0.0005)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, 'max', factor = 0.1, patience = 5, verbose=True)\n",
    "\n",
    "train_loader, test_loader = create_dataloaders(train_images, train_labels,n_test_images,n_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "EPOCH = 10\n",
    "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "    ocean.train().cuda()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs = inputs.cuda()\n",
    "        lbs = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = ocean(inputs)\n",
    "        loss = criterion(outputs, lbs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "    ocean.eval().cuda()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for (images, labels) in test_loader:\n",
    "        images = images.cuda()\n",
    "        labels=labels[::4]\n",
    "        labels = labels.cuda()\n",
    "        #print(images)\n",
    "        outputs = ocean(images)\n",
    "        outputs1=[]\n",
    "        for i in range(0,len(outputs),4):\n",
    "            c = torch.sum(outputs[i:i+4],0)\n",
    "            outputs1.append(c)\n",
    "        outputs1=torch.stack(outputs1)\n",
    "        _, predicted = torch.max(outputs1.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on EPOCH %d test images: %d %%' % (epoch+1, 100 * correct / total))   \n",
    "    lr_scheduler.step(correct / total)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "jj=0\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        for (images, labels) in test_loader:\n",
    "            try:\n",
    "                jj+=1\n",
    "                #print(\"hi\")\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                #print(images)\n",
    "                #input()\n",
    "                outputs = ocean(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                #print(\"here\")\n",
    "            except:\n",
    "                print(jj)\n",
    "    except:\n",
    "        print(jj)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "print(correct)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ValidationImagesDataset(n_val_images, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "i=0\n",
    "for images in val_loader:\n",
    "    i+=1\n",
    "    outputs = ocean(images.cuda())\n",
    "    #print(outputs.size())\n",
    "    #print(outputs[0])\n",
    "    #input()outputs1=[]\n",
    "    for i in range(0,len(outputs),4):\n",
    "        c = torch.sum(outputs[i:i+4],0)\n",
    "        outputs1.append(c)\n",
    "    outputs1=torch.stack(outputs1)\n",
    "    _, predicted = torch.max(outputs1.data, 1)\n",
    "    prediction.extend(predicted.cpu().numpy())\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'image': val_fname, 'class': prediction}, columns=['image', 'class'])\n",
    "results_df.to_csv(os.getcwd()+\"/results_new.csv\", sep = ',', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
